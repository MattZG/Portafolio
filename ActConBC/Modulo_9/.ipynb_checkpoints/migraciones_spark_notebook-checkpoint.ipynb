{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d53fb9a",
   "metadata": {},
   "source": [
    "# Análisis de migraciones con Spark\n",
    "\n",
    "Este notebook contiene las instrucciones y el código (celdas **Markdown** y **Código**) para realizar las tareas solicitadas sobre el archivo `/mnt/data/migraciones.csv` usando **PySpark**.\n",
    "\n",
    "**Notas:**\n",
    "\n",
    "- Diseñado para ejecutarse en un entorno con `pyspark` disponible (por ejemplo, un cluster Spark, Databricks, o una instalación local con PySpark).  \n",
    "- Si `pyspark` no está instalado, las celdas incluyen mensajes guía para instalar/activar el entorno.  \n",
    "- Rutas usadas: dataset en `/mnt/data/migraciones.csv`, salida Parquet en `/mnt/data/migraciones_parquet/`.\n",
    "\n",
    "Ejecute las celdas en orden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea788d",
   "metadata": {},
   "source": [
    "## 0. Preparar entorno y crear SparkSession\n",
    "\n",
    "Esta celda crea un `SparkSession`. Si no tiene `pyspark` instalado, aparecerá un error y la celda muestra instrucciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87354da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Celda: crear SparkSession\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MigracionesAnalysis\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"SparkSession iniciada correctamente: \", spark)\n",
    "except Exception as e:\n",
    "    print(\"Error al iniciar SparkSession. Asegúrese de tener pyspark instalado y configurado.\")\n",
    "    print(\"Error:\", e)\n",
    "    print(\"\\nSi está en un entorno local, puede instalar pyspark con: pip install pyspark\")\n",
    "    print(\"En entornos gestionados (Databricks, EMR, GCP Dataproc, etc.) no necesita instalar pyspark.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4bda96",
   "metadata": {},
   "source": [
    "## 1. Carga y exploración de datos (2 puntos)\n",
    "\n",
    "- Cargue el CSV en Spark.\n",
    "- Convierta a RDD y DataFrame.\n",
    "- Muestre primeras filas, esquema y estadísticas descriptivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f543e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Celda: leer CSV como DataFrame de Spark\n",
    "csv_path = \"/mnt/data/migraciones.csv\"\n",
    "\n",
    "# Intentamos leer el CSV con inferSchema para facilitar el análisis.\n",
    "try:\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(csv_path)\n",
    "    print(\"Dataset cargado en DataFrame 'df' con {} filas\".format(df.count()))\n",
    "except Exception as e:\n",
    "    print(\"Error al leer el CSV. Verifique la ruta y el formato. Error:\", e)\n",
    "    df = None\n",
    "\n",
    "# Convertir a RDD\n",
    "if df is not None:\n",
    "    rdd = df.rdd\n",
    "    print(\"Se generó rdd a partir de DataFrame.\")\n",
    "else:\n",
    "    rdd = None\n",
    "\n",
    "# Mostrar primeras filas\n",
    "if df is not None:\n",
    "    display(df.limit(10).toPandas())  # muestra las primeras 10 filas (convierte a pandas para mejor visualización)\n",
    "    print('\\nEsquema del DataFrame:')\n",
    "    df.printSchema()\n",
    "\n",
    "    print('\\nEstadísticas descriptivas (describe()):')\n",
    "    display(df.describe().toPandas())\n",
    "else:\n",
    "    print('DataFrame no disponible.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4bc249",
   "metadata": {},
   "source": [
    "## 2. Procesamiento con RDDs y DataFrames (3 puntos)\n",
    "\n",
    "### 2.1 Transformaciones y acciones con RDDs\n",
    "\n",
    "Ejemplos de `filter`, `map`, `flatMap` y acciones `collect`, `take`, `count`. Ajuste las columnas según su dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Celda: operaciones con RDD (ejemplos)\n",
    "if rdd is not None:\n",
    "    # Mostrar esquema de rdd (primer elemento)\n",
    "    first = rdd.first()\n",
    "    print(\"Primer registro (tupla Row):\", first)\n",
    "\n",
    "    # Ejemplo: filtrar filas cuyo país origen no sea nulo (ajuste 'pais_origen' al nombre real)\n",
    "    # Intentamos detectar columnas comunes\n",
    "    cols = df.columns\n",
    "    print(\"Columnas detectadas:\", cols)\n",
    "\n",
    "    # Buscamos columnas con nombres parecidos a 'origen' y 'destino'\n",
    "    origen_cols = [c for c in cols if 'origen' in c.lower() or 'or'==c.lower()]\n",
    "    destino_cols = [c for c in cols if 'destino' in c.lower() or 'dest'==c.lower()]\n",
    "    print(\"Posibles columnas de origen:\", origen_cols)\n",
    "    print(\"Posibles columnas de destino:\", destino_cols)\n",
    "\n",
    "    # Use la primera columna candidata si existe, sino use la primera columna del dataset (solo ejemplo)\n",
    "    origen_col = origen_cols[0] if origen_cols else cols[0]\n",
    "    destino_col = destino_cols[0] if destino_cols else (cols[1] if len(cols)>1 else cols[0])\n",
    "\n",
    "    print(\"Usando para ejemplos:\", origen_col, destino_col)\n",
    "\n",
    "    # filter: conservar registros donde origen no sea nulo\n",
    "    filtered = rdd.filter(lambda row: row[origen_col] is not None and str(row[origen_col]).strip()!='')\n",
    "    print(\"Registros con origen no nulo:\", filtered.count())\n",
    "\n",
    "    # map: crear pares (origen, 1)\n",
    "    mapped = filtered.map(lambda row: (row[origen_col], 1))\n",
    "\n",
    "    # flatMap: si hubiera una columna con múltiples razones separadas por ';', dividirlas\n",
    "    # buscamos columna con 'razon' en el nombre\n",
    "    razon_cols = [c for c in cols if 'razon' in c.lower() or 'motivo' in c.lower()]\n",
    "    print(\"Posibles columnas de razón de migración:\", razon_cols)\n",
    "    if razon_cols:\n",
    "        razon_col = razon_cols[0]\n",
    "        flat = rdd.flatMap(lambda row: str(row[razon_col]).split(';') if row[razon_col] else [])\n",
    "        print(\"Ejemplo flatMap (primeros 10):\", flat.take(10))\n",
    "    else:\n",
    "        print(\"No se encontró columna de razón para flatMap de ejemplo.\")\n",
    "\n",
    "    # acciones: collect (¡cuidado con tamaños grandes!), take, count\n",
    "    print(\"Ejemplo take(5):\", mapped.take(5))\n",
    "else:\n",
    "    print(\"RDD no disponible.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770444f",
   "metadata": {},
   "source": [
    "### 2.2 Operaciones con DataFrames: filtrado, agregaciones y ordenamiento\n",
    "\n",
    "Además, se guardan resultados en formato Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813342a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Celda: operaciones con DataFrame\n",
    "import os\n",
    "output_parquet = \"/mnt/data/migraciones_parquet\"\n",
    "\n",
    "if df is not None:\n",
    "    # Filtrar: ejemplo conservando migraciones después de cierto año si existe 'anio' o 'year'\n",
    "    year_cols = [c for c in df.columns if 'anio' in c.lower() or 'year' in c.lower() or 'año' in c.lower()]\n",
    "    print(\"Posibles columnas de año:\", year_cols)\n",
    "    if year_cols:\n",
    "        year_col = year_cols[0]\n",
    "        try:\n",
    "            df_filtered = df.filter(df[year_col] >= 2000)  # ejemplo\n",
    "            print(\"Filtrado por año >= 2000. Registros:\", df_filtered.count())\n",
    "        except Exception as e:\n",
    "            print(\"No se pudo filtrar por año (tipo/valores). Error:\", e)\n",
    "            df_filtered = df\n",
    "    else:\n",
    "        df_filtered = df\n",
    "\n",
    "    # Agregaciones: top países origen y destino\n",
    "    # Usamos las columnas detectadas antes (origen_col, destino_col)\n",
    "    top_origen = df.groupBy(origen_col).count().orderBy('count', ascending=False)\n",
    "    top_destino = df.groupBy(destino_col).count().orderBy('count', ascending=False)\n",
    "\n",
    "    print(\"Top 10 países de origen:\")\n",
    "    display(top_origen.limit(10).toPandas())\n",
    "\n",
    "    print(\"Top 10 países de destino:\")\n",
    "    display(top_destino.limit(10).toPandas())\n",
    "\n",
    "    # Guardar en Parquet (sobrescribe si existe)\n",
    "    try:\n",
    "        if os.path.exists(output_parquet):\n",
    "            print(\"Directorio Parquet ya existe en {}. Se recomienda borrarlo manualmente si desea sobrescribir.\".format(output_parquet))\n",
    "        else:\n",
    "            df.write.mode('overwrite').parquet(output_parquet)\n",
    "            print(\"DataFrame escrito en Parquet en:\", output_parquet)\n",
    "    except Exception as e:\n",
    "        print(\"Error al escribir Parquet:\", e)\n",
    "else:\n",
    "    print(\"DataFrame no disponible para operaciones.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc855b",
   "metadata": {},
   "source": [
    "## 3. Consultas con Spark SQL (2 puntos)\n",
    "\n",
    "- Registrar el DataFrame como una tabla temporal.\n",
    "- Consultas sobre principales países de origen/destino.\n",
    "- Análisis de razones de migración por región."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Celda: consultas SQL\n",
    "if df is not None:\n",
    "    df.createOrReplaceTempView(\"migraciones\")\n",
    "    print(\"Tabla temporal 'migraciones' registrada. Ahora puede ejecutar consultas SQL.\")\n",
    "\n",
    "    # Top países origen\n",
    "    q1 = \"\"\"SELECT {o} AS origen, COUNT(*) AS cnt\n",
    "           FROM migraciones\n",
    "           GROUP BY {o}\n",
    "           ORDER BY cnt DESC\n",
    "           LIMIT 10\"\"\".format(o=origen_col)\n",
    "    print(\"\\nTop países de origen (SQL):\")\n",
    "    display(spark.sql(q1).toPandas())\n",
    "\n",
    "    # Top países destino\n",
    "    q2 = \"\"\"SELECT {d} AS destino, COUNT(*) AS cnt\n",
    "           FROM migraciones\n",
    "           GROUP BY {d}\n",
    "           ORDER BY cnt DESC\n",
    "           LIMIT 10\"\"\".format(d=destino_col)\n",
    "    print(\"\\nTop países de destino (SQL):\")\n",
    "    display(spark.sql(q2).toPandas())\n",
    "\n",
    "    # Razones por región: buscamos columnas 'region' y 'razon' si existen\n",
    "    region_cols = [c for c in df.columns if 'region' in c.lower()]\n",
    "    if region_cols and razon_cols:\n",
    "        region_col = region_cols[0]\n",
    "        razon_col = razon_cols[0]\n",
    "        q3 = \"\"\"SELECT {reg} AS region, {raz} AS razon, COUNT(*) AS cnt\n",
    "               FROM migraciones\n",
    "               GROUP BY {reg}, {raz}\n",
    "               ORDER BY {reg}, cnt DESC\n",
    "               LIMIT 50\"\"\".format(reg=region_col, raz=razon_col)\n",
    "        print(\"\\nPrincipales razones por región:\")\n",
    "        display(spark.sql(q3).toPandas())\n",
    "    else:\n",
    "        print(\"No se encontraron columnas 'region' y/o 'razon' para el análisis. Columnas disponibles:\", df.columns)\n",
    "else:\n",
    "    print(\"DataFrame no disponible para SQL.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf621d2",
   "metadata": {},
   "source": [
    "## 4. Aplicación de MLlib para predicción de flujos migratorios (3 puntos)\n",
    "\n",
    "- Convertir datos a formato MLlib.\n",
    "- Aplicar Regression (Logistic Regression) para predecir probabilidad de migración.\n",
    "- Evaluar el modelo.\n",
    "\n",
    "**Importante:** el notebook trata de detectar automáticamente una columna etiqueta (por ejemplo `migracion`, `migrante`, `target`). Si no la detecta, edite la variable `label_col` en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7fdf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Celda: preparación para MLlib y entrenamiento (ejemplo)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Detectar automáticamente una columna label (heurística)\n",
    "label_candidates = [c for c in df.columns if any(k in c.lower() for k in ['mig', 'migrant', 'migracion', 'migrate', 'target', 'label'])]\n",
    "if label_candidates:\n",
    "    label_col = label_candidates[0]\n",
    "    print(\"Label detectada automáticamente:\", label_col)\n",
    "else:\n",
    "    # Si no encontramos una label, pedir al usuario que edite esta variable manualmente.\n",
    "    label_col = None\n",
    "    print(\"No se detectó columna etiqueta automáticamente. Por favor, asigne manualmente `label_col` al nombre de la columna objetivo. Columnas disponibles:\", df.columns)\n",
    "\n",
    "# Supongamos que label_col es binaria (0/1). Si no es así habrá que transformarla.\n",
    "if label_col:\n",
    "    # Seleccionar features numéricas automáticamente\n",
    "    numeric_cols = [f.name for f in df.schema.fields if str(f.dataType) in ['IntegerType', 'DoubleType', 'LongType', 'FloatType'] and f.name != label_col]\n",
    "    print(\"Características numéricas detectadas:\", numeric_cols)\n",
    "\n",
    "    if not numeric_cols:\n",
    "        print(\"No se detectaron columnas numéricas. Debe preprocesar variables categóricas antes de usar modelos que requieran vectores numéricos.\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "\n",
    "    # Transformar label si es string\n",
    "    if dict(df.dtypes)[label_col] not in ('int', 'double', 'float', 'bigint'):\n",
    "        indexer = StringIndexer(inputCol=label_col, outputCol=\"label_indexed\", handleInvalid='keep')\n",
    "        label_used = \"label_indexed\"\n",
    "        pipeline_stages = [indexer, assembler]\n",
    "    else:\n",
    "        label_used = label_col\n",
    "        pipeline_stages = [assembler]\n",
    "\n",
    "    # Add logistic regression to pipeline\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=label_used, predictionCol=\"prediction\", maxIter=20)\n",
    "    pipeline_stages.append(lr)\n",
    "    pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "    # Split data\n",
    "    train, test = df.randomSplit([0.7, 0.3], seed=42)\n",
    "    print(\"Train/Test counts:\", train.count(), test.count())\n",
    "\n",
    "    # Fit model\n",
    "    model = pipeline.fit(train)\n",
    "    print(\"Modelo entrenado.\")\n",
    "\n",
    "    # Predicciones\n",
    "    preds = model.transform(test)\n",
    "    print(\"Predicciones realizadas. Ejemplo:\")\n",
    "    display(preds.select(label_used, \"prediction\", \"probability\").limit(10).toPandas())\n",
    "\n",
    "    # Evaluación\n",
    "    # Si problema binario usamos BinaryClassificationEvaluator sobre areaUnderROC\n",
    "    try:\n",
    "        evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=label_used, metricName=\"areaUnderROC\")\n",
    "        auc = evaluator.evaluate(preds)\n",
    "        print(\"AUC (areaUnderROC):\", auc)\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo calcular AUC (posiblemente label no binaria). Error:\", e)\n",
    "\n",
    "    # Matriz de confusión (conteos por etiqueta/predicción)\n",
    "    cm = preds.groupBy(label_used, \"prediction\").count().orderBy(label_used, \"prediction\")\n",
    "    print(\"Matriz de confusión (conteos):\")\n",
    "    display(cm.toPandas())\n",
    "\n",
    "else:\n",
    "    print(\"No hay columna etiqueta definida. Edite `label_col` en esta celda para continuar con el pipeline de ML.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41b318",
   "metadata": {},
   "source": [
    "## Conclusiones y pasos siguientes\n",
    "\n",
    "- El notebook cubre la carga, exploración, transformaciones con RDDs, operaciones con DataFrames, consultas SQL y un pipeline de ejemplo con MLlib.\n",
    "- **Sugerencias para ejecución real:**\n",
    "  - Asegúrese de ejecutar en un entorno con `pyspark` y memoria adecuada.  \n",
    "  - Revise/ajuste los nombres de columnas (`origen`, `destino`, `razon`, `region`, `label`) según su archivo real.  \n",
    "  - Para variables categóricas, incluya `StringIndexer` + `OneHotEncoder` según convenga.  \n",
    "  - Para grandes datasets, evite `collect()` y prefiera `limit()` o `take()`.\n",
    "\n",
    "Si quiere, puedo:  \n",
    "- Adaptarlo para ejecutar con `pandas` si prefiere (más liviano para computadoras locales sin Spark)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
